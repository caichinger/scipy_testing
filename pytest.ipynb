{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest\n",
    "\n",
    "> The [pytest](https://docs.pytest.org/en/latest/) framework makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest\n",
    "\n",
    "\n",
    "- Tests without boilerplate (no need for `self.assert*` or classes)\n",
    "\n",
    "\n",
    "- Very powerful\n",
    "\n",
    "\n",
    "- Rich plugin architecture\n",
    "\n",
    "\n",
    "- Can run unittests, nosetests, doctests, works well with hypothesis\n",
    "\n",
    "\n",
    "- Mature\n",
    "\n",
    "---\n",
    "\n",
    "We will only take a look at a few features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_sample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_sample.py\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def test_answer():\n",
    "    assert inc(3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 1 item                                                                \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_sample.py F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_answer __________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_answer():\u001b[0m\n",
      "\u001b[1m>       assert inc(3) == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 4 = inc(3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_sample.py\u001b[0m:6: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.08 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good to Know and Do\n",
    "\n",
    "### [Fixtures](https://docs.pytest.org/en/latest/fixture.html#fixtures)\n",
    "\n",
    "> The purpose of test fixtures is to provide a fixed baseline upon which tests can reliably and repeatedly execute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_fixture.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def create_data():\n",
    "    return 'We need that'\n",
    "\n",
    "def test_needs(create_data):\n",
    "    assert create_data == 'Do we need that?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 1 item                                                                \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_fixture.py F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_needs __________________________________\u001b[0m\n",
      "\n",
      "create_data = 'We need that'\n",
      "\n",
      "\u001b[1m    def test_needs(create_data):\u001b[0m\n",
      "\u001b[1m>       assert create_data == 'Do we need that?'\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'We need that' == 'Do we need that?'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - We need that\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ^\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + Do we need that?\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ^^^^           +\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_fixture.py\u001b[0m:8: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.06 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asserting exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_exception.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_exception.py\n",
    "import pytest\n",
    "\n",
    "def fun():\n",
    "    raise ZeroDivisionError(1)\n",
    "\n",
    "def test_fun_raises_zero_division_error():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        fun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 1 item                                                                \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_exception.py .\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Parametrizing test functions](https://docs.pytest.org/en/latest/parametrize.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_expectation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_expectation.py\n",
    "\n",
    "# content of test_expectation.py\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"test_input,expected\", [\n",
    "    (\"3+5\", 8),\n",
    "    (\"2+4\", 6),\n",
    "    (\"6*9\", 42),\n",
    "])\n",
    "def test_eval(test_input, expected):\n",
    "    assert eval(test_input) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 3 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_expectation.py ..F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_eval[6*9-42] _______________________________\u001b[0m\n",
      "\n",
      "test_input = '6*9', expected = 42\n",
      "\n",
      "\u001b[1m    @pytest.mark.parametrize(\"test_input,expected\", [\u001b[0m\n",
      "\u001b[1m        (\"3+5\", 8),\u001b[0m\n",
      "\u001b[1m        (\"2+4\", 6),\u001b[0m\n",
      "\u001b[1m        (\"6*9\", 42),\u001b[0m\n",
      "\u001b[1m    ])\u001b[0m\n",
      "\u001b[1m    def test_eval(test_input, expected):\u001b[0m\n",
      "\u001b[1m>       assert eval(test_input) == expected\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 54 == 42\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 54 = eval('6*9')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_expectation.py\u001b[0m:10: AssertionError\n",
      "\u001b[31m\u001b[1m====================== 1 failed, 2 passed in 0.07 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_expectation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/test_tmpdir.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/test_tmpdir.py\n",
    "import os\n",
    "\n",
    "def test_create_file(tmpdir):\n",
    "    p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n",
    "    p.write(\"content\")\n",
    "    assert p.read() == \"content\"\n",
    "    assert len(tmpdir.listdir()) == 1\n",
    "    assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 1 item                                                                \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/test_tmpdir.py F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_create_file _______________________________\u001b[0m\n",
      "\n",
      "tmpdir = local('/tmp/pytest-of-claus/pytest-5/test_create_file0')\n",
      "\n",
      "\u001b[1m    def test_create_file(tmpdir):\u001b[0m\n",
      "\u001b[1m        p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\u001b[0m\n",
      "\u001b[1m        p.write(\"content\")\u001b[0m\n",
      "\u001b[1m        assert p.read() == \"content\"\u001b[0m\n",
      "\u001b[1m        assert len(tmpdir.listdir()) == 1\u001b[0m\n",
      "\u001b[1m>       assert 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpytest/test_tmpdir.py\u001b[0m:8: AssertionError\n",
      "\u001b[31m\u001b[1m=========================== 1 failed in 0.07 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/test_tmpdir.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy pandas test stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytest-watch\n",
    "\n",
    "> Local continuous test runner with pytest\n",
    "\n",
    "```\n",
    "$ cd myproject\n",
    "$ ptw\n",
    " * Watching /path/to/myproject\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "- Go to `./pytest/watch` and use `pytest-watch` to monitor the respective files. Try out pytest's failure reporting on strings, lists and dictionaries by implementing corresponding test cases.\n",
    "\n",
    "\n",
    "- Implement a parametrized pytest test case for the doctested integration function to test against several inputs.\n",
    "\n",
    "\n",
    "- Implement the Exception-handling-testcase from the doctest exercises with pytest. \n",
    "\n",
    "\n",
    "- Export a csv file to a temporary directory for use in a test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest/solutions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest/solutions.py\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#%%\n",
    "@pytest.fixture\n",
    "def create_df():\n",
    "    df = pd.DataFrame({'a': [1, 2], \n",
    "                       'b': [2, 3]})\n",
    "    return df\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def create_data(tmpdir, create_df):\n",
    "    df = create_df\n",
    "    path = tmpdir.join('df.csv')\n",
    "    df.to_csv(path, index=None)\n",
    "    return path\n",
    "\n",
    "\n",
    "def test_with_tmpdir_fixture(create_data, create_df):\n",
    "    df_expected = create_df\n",
    "    path = create_data\n",
    "    df_actual = pd.read_csv(path)\n",
    "    pd.testing.assert_frame_equal(df_actual, df_expected)\n",
    "\n",
    "\n",
    "#%%\n",
    "def integrate(f, x):\n",
    "    y = f(x)\n",
    "    return np.sum((y[:-1] + y[1:]) * np.diff(x)) / 2\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize('fun,integral', [(lambda x: x, 0.5), \n",
    "                                          (lambda x: 0*x + 1, 1)])\n",
    "def test_integrate(fun, integral):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    assert integrate(fun, x) == integral\n",
    "\n",
    "\n",
    "#%%\n",
    "def some_exception():\n",
    "    raise ZeroDivisionError('O.o')\n",
    "\n",
    "\n",
    "def test_some_exception():\n",
    "    with pytest.raises(ZeroDivisionError) as excinfo:\n",
    "        some_exception()\n",
    "    assert excinfo.value.args[0] == 'O.o' \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pytest.main([__file__])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /home/claus/repo/SCIPYTESTING, inifile:\n",
      "plugins: mock-1.6.0, cov-2.5.1, hypothesis-3.33.0\n",
      "collected 4 items                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pytest/solutions.py ....\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 4 passed in 0.66 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest/solutions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- pytest is worth learning\n",
    "\n",
    "\n",
    "- Often, most test cases are rather simple, pytest is great for that\n",
    "\n",
    "\n",
    "- pytest is also great for tackling complicated testing scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
